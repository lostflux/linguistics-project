{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYgIn9dcBPmf",
        "outputId": "712294b1-53f7-4d12-8de8-d5ccdc12954e"
      },
      "outputs": [],
      "source": [
        "# %pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TuOzAxpATzY1"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pytorch_lightning as pl\n",
        "import torchaudio\n",
        "import torchmetrics\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT7lBemwZWm2",
        "outputId": "ba42628d-f940-4527-bc92-6991ce7a20a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xytqEb8a_1Vi"
      },
      "outputs": [],
      "source": [
        "class SpeechEmotionRecognitionModel(pl.LightningModule):\n",
        "    def __init__(self, input_size, num_classes, dim_feedforward=2048, dim_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, lr=1e-2, dropout=0.1):\n",
        "        super(SpeechEmotionRecognitionModel, self).__init__()\n",
        "        self.lr = lr\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(dim_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
        "        self.encoder = nn.Linear(input_size, dim_model)\n",
        "        self.decoder = nn.Linear(dim_model, num_classes)\n",
        "        \n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "        # initialize the metrics\n",
        "        self.precision = torchmetrics.Precision(task='multiclass', num_classes=num_classes, average=\"macro\")\n",
        "        self.recall = torchmetrics.Recall(task='multiclass', num_classes=num_classes, average=\"macro\")\n",
        "        self.F1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average=\"macro\")\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.unsqueeze(1)  # Add batch dimension\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = output.squeeze(1)  # Remove the batch dimension\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src, tgt = batch[0], batch[1]\n",
        "        output = self(src)\n",
        "        loss = self.loss_function(output, tgt)\n",
        "        self.log('cross entropy loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(src)\n",
        "        loss = self.loss_function(output, tgt.float())\n",
        "        self.log('cross entropy loss', loss, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = {\n",
        "            'scheduler': ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True),\n",
        "            'monitor': 'cross entropy loss_epoch',  # Name of the metric to monitor\n",
        "            'interval': 'epoch',\n",
        "            'frequency': 1,\n",
        "        }\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
        "\n",
        "    # function for evaluating the quality of output and target\n",
        "    def evaluation(self, output, target, loss):\n",
        "        # Calculate and log metrics\n",
        "        output = output.squeeze()\n",
        "        print('precision:{}'.format(self.precision(output, target)))\n",
        "        self.log('recall:{}'.format(self.recall(output, target)))\n",
        "        self.log('f1:{}'.format(self.F1(output, target)))\n",
        "\n",
        "        # Calculate the loss\n",
        "        print(\"cross entropy loss:{}\".format(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from local_dataset import AudioEmotionsDataset \n",
        "# import TQDMProgressBar\n",
        "from pytorch_lightning.callbacks import TQDMProgressBar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_len = 114220\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset = AudioEmotionsDataset(\"data/audio-emotions\", batch_size=BATCH_SIZE)\n",
        "\n",
        "train = dataset.train_dataloader\n",
        "test = dataset.test_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/amittaijoel/miniconda3/envs/ling-project/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "model = SpeechEmotionRecognitionModel(input_size=dataset.feature_count, num_classes=dataset.class_count)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "trainer = pl.Trainer(default_root_dir='checkpoints',callbacks=[TQDMProgressBar(refresh_rate=10)], accelerator=\"auto\", max_epochs=50, min_epochs=10, log_every_n_steps=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/amittaijoel/miniconda3/envs/ling-project/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "\n",
            "  | Name                | Type                | Params\n",
            "------------------------------------------------------------\n",
            "0 | transformer_encoder | TransformerEncoder  | 50.4 M\n",
            "1 | encoder             | Linear              | 116 M \n",
            "2 | decoder             | Linear              | 7.2 K \n",
            "3 | loss_function       | CrossEntropyLoss    | 0     \n",
            "4 | precision           | MulticlassPrecision | 0     \n",
            "5 | recall              | MulticlassRecall    | 0     \n",
            "6 | F1                  | MulticlassF1Score   | 0     \n",
            "------------------------------------------------------------\n",
            "167 M     Trainable params\n",
            "0         Non-trainable params\n",
            "167 M     Total params\n",
            "669.475   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6948222c8c1b4fcb912f0aba1b351ace",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/amittaijoel/miniconda3/envs/ling-project/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/Users/amittaijoel/miniconda3/envs/ling-project/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
            "/Users/amittaijoel/miniconda3/envs/ling-project/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6418f748be141959f46dbd1874d5483",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/amittaijoel/miniconda3/envs/ling-project/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model, train_dataloaders=train, val_dataloaders=test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
