{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_dataset import AudioEmotionsDataset\n",
    "from IPython.display import Audio, display, Markdown\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, Wav2Vec2Processor\n",
    "\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "class AudioEmotionsDataset():\n",
    "    def __init__(self, data_path=None, train_split=0.8, batch_size=64, max_size=None):\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.data_path = data_path if data_path is not None else \"data/audio-emotions\"\n",
    "        self.train_split = train_split\n",
    "        \n",
    "        # if path does not exist, download the dataset and extract\n",
    "        if not os.path.exists(self.data_path):\n",
    "            raise Exception(f\"\"\"\n",
    "                Dataset path not found at {self.data_path}.\n",
    "                Please download the dataset from https://www.kaggle.com/uldisvalainis/audio-emotions\n",
    "                and save it in the relevant path.\n",
    "                \"\"\")\n",
    "        \n",
    "        metadata = {\n",
    "            \"angry\": sorted(glob(f\"{self.data_path}/Angry/*.wav\")),\n",
    "            \"sad\": sorted(glob(f\"{self.data_path}/Sad/*.wav\")),\n",
    "            \"disgusted\": sorted(glob(f\"{self.data_path}/Disgusted/*.wav\")),\n",
    "            \"fearful\": sorted(glob(f\"{self.data_path}/Fearful/*.wav\")),\n",
    "            \"happy\": sorted(glob(f\"{self.data_path}/Happy/*.wav\")),\n",
    "            \"neutral\": sorted(glob(f\"{self.data_path}/Neutral/*.wav\")),\n",
    "            \"surprised\": sorted(glob(f\"{self.data_path}/Surprised/*.wav\")),\n",
    "        }\n",
    "\n",
    "        train_meta = {\n",
    "            \"angry\": metadata[\"angry\"][:int(self.train_split * len(metadata[\"angry\"]))],\n",
    "            \"sad\": metadata[\"sad\"][:int(self.train_split * len(metadata[\"sad\"]))],\n",
    "            \"disgusted\": metadata[\"disgusted\"][:int(self.train_split * len(metadata[\"disgusted\"]))],\n",
    "            \"fearful\": metadata[\"fearful\"][:int(self.train_split * len(metadata[\"fearful\"]))],\n",
    "            \"happy\": metadata[\"happy\"][:int(self.train_split * len(metadata[\"happy\"]))],\n",
    "            \"neutral\": metadata[\"neutral\"][:int(self.train_split * len(metadata[\"neutral\"]))],\n",
    "            \"surprised\": metadata[\"surprised\"][:int(self.train_split * len(metadata[\"surprised\"]))],\n",
    "        }\n",
    "\n",
    "        test_meta = {\n",
    "            \"angry\": metadata[\"angry\"][int(self.train_split * len(metadata[\"angry\"])):],\n",
    "            \"sad\": metadata[\"sad\"][int(self.train_split * len(metadata[\"sad\"])):],\n",
    "            \"disgusted\": metadata[\"disgusted\"][int(self.train_split * len(metadata[\"disgusted\"])):],\n",
    "            \"fearful\": metadata[\"fearful\"][int(self.train_split * len(metadata[\"fearful\"])):],\n",
    "            \"happy\": metadata[\"happy\"][int(self.train_split * len(metadata[\"happy\"])):],\n",
    "            \"neutral\": metadata[\"neutral\"][int(self.train_split * len(metadata[\"neutral\"])):],\n",
    "            \"surprised\": metadata[\"surprised\"][int(self.train_split * len(metadata[\"surprised\"])):],\n",
    "        }\n",
    "        \n",
    "        # limit to size\n",
    "        if max_size is not None:\n",
    "            for emotion in train_meta:\n",
    "                train_meta[emotion] = train_meta[emotion][:max_size]\n",
    "            for emotion in test_meta:\n",
    "                test_meta[emotion] = test_meta[emotion][:max_size]\n",
    "        \n",
    "        waveforms_train, X_train, y_train = [], [], []\n",
    "        \n",
    "        # NOTE: changing the class map here will change the numerical values for the classes.\n",
    "        self.class_map = {0: \"angry\", 1: \"sad\", 2: \"disgusted\", 3: \"fearful\", 4: \"happy\", 5: \"neutral\", 6: \"surprised\"}\n",
    "        self.class_map_inv = {v: k for k, v in self.class_map.items()}\n",
    "        \n",
    "        for emotion in train_meta:\n",
    "            for data_point in train_meta[emotion]:\n",
    "                waveform, features = self.extract_features(data_point)\n",
    "                waveforms_train.append(waveform)\n",
    "                X_train.append(features)\n",
    "                \n",
    "            y_train += [self.class_map_inv[emotion]] * len(train_meta[emotion])\n",
    "            \n",
    "        waveforms_test, X_test, y_test = [], [], []\n",
    "        for emotion in test_meta:\n",
    "            for data_point in test_meta[emotion]:\n",
    "                waveform, features = self.extract_features(data_point)\n",
    "                waveforms_test.append(waveform)\n",
    "                X_test.append(features)\n",
    "                \n",
    "            y_test += [self.class_map_inv[emotion]] * len(test_meta[emotion])\n",
    "            \n",
    "        # ZERO-PADDING\n",
    "        # pad the features to the same length\n",
    "        max_len = max([x.shape[1] for x in X_train + X_test])\n",
    "        print(f\"{max_len = }\")\n",
    "        \n",
    "        for i in range(len(X_train)):\n",
    "            m = nn.ZeroPad2d((0, max_len - X_train[i].shape[1]))\n",
    "            X_train[i] = m(X_train[i])\n",
    "            \n",
    "        # shuffle uniformly\n",
    "        # self.waveforms_train, self.X_train, self.y_train = self.shuffle_datapoints(waveforms_train, X_train, y_train)\n",
    "        # self.waveforms_test, self.X_test, self.y_test = self.shuffle_datapoints(waveforms_test, X_test, y_test)\n",
    "        \n",
    "        # zip train x and y\n",
    "        zipped_train = list(zip(self.X_train, self.y_train))\n",
    "        zipped_test = list(zip(self.X_test, self.y_test))\n",
    "        \n",
    "        self.train_dataloader = DataLoader(zipped_train, batch_size=batch_size, shuffle=True)\n",
    "        self.test_dataloader = DataLoader(zipped_test, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    def extract_features(self, file: str):\n",
    "        \"\"\"\n",
    "            Loads audio from a given file path and extracts features using the Wav2Vec2 model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # NOTE: Wav2Vec2 was trained with a sampling rate of 16kHz,\n",
    "        # so we need to resample the audio files to 16kHz.\n",
    "        waveform, sample_rate = librosa.load(file, sr=16000)\n",
    "        \n",
    "        \n",
    "        #? extract features\n",
    "        features = self.feature_extractor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "        \n",
    "        return waveform, features\n",
    "    \n",
    "    def shuffle_datapoints(self, waveforms, X, y):\n",
    "        \"\"\"\n",
    "            Shuffles the data points uniformly.\n",
    "        \"\"\"\n",
    "        zipped = list(zip(waveforms, X, y))\n",
    "        random.shuffle(zipped)\n",
    "        waveforms, X, y = zip(*zipped)\n",
    "        return waveforms, X, y\n",
    "    \n",
    "    def transcribe(self, features):\n",
    "        \"\"\"\n",
    "            (Useful for testing)\n",
    "            Generate a transcription from `Wav2Vec2` features.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            - word_offsets: list of dicts for each word with start and end times\n",
    "            - transcription: the transcription of the audio\n",
    "        \"\"\"\n",
    "        \n",
    "        input_values = features # torch.tensor(features)\n",
    "        logits = self.model(input_values).logits[0]\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        outputs = self.tokenizer.decode(predicted_ids, output_word_offsets=True)\n",
    "        time_offset = self.model.config.inputs_to_logits_ratio / self.feature_extractor.sampling_rate\n",
    "        \n",
    "        word_offsets = [\n",
    "            { \"word\": d[\"word\"],\n",
    "                \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n",
    "                \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n",
    "            }\n",
    "            for d in outputs.word_offsets\n",
    "        ]\n",
    "        \n",
    "        transcription = \" \".join([ item['word'] for item in word_offsets])\n",
    "\n",
    "        return word_offsets, transcription\n",
    "    \n",
    "    def one_hot_encode(self, y):\n",
    "        \"\"\"\n",
    "            One-hot encode a single datapoint.\n",
    "            (Maybe do this all at once for the entire dataset?)\n",
    "        \"\"\"\n",
    "        return F.one_hot(torch.tensor(y), num_classes=len(self.class_map))\n",
    "\n",
    "    def len_train(self):\n",
    "        \"\"\"\n",
    "            Get the length of the training split of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def len_test(self):\n",
    "        \"\"\"\n",
    "            get the length of the testing split of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.y_test)\n",
    "    \n",
    "    def get_train(self, idx):\n",
    "        \"\"\"\n",
    "            Get a single training datapoint.\n",
    "            \n",
    "            if iterating, try accessing `dataset.X_train` and `dataset.y_train` directly.\n",
    "        \"\"\"\n",
    "        return self.waveforms_train[idx], self.X_train[idx], self.y_train[idx]\n",
    "    \n",
    "    def get_test(self, idx):\n",
    "        \"\"\"\n",
    "            Get a single testing datapoint.\n",
    "            \n",
    "            If iterating, try accessing `dataset.X_test` and `dataset.y_test` directly.\n",
    "        \"\"\"\n",
    "        return self.waveforms_test[idx], self.X_test[idx], self.y_test[idx]\n",
    "    \n",
    "    def label_to_num(self, emotion):\n",
    "        \"\"\"\n",
    "            Encode a label (e.g. \"angry\") to a numerical value (e.g. 0)\n",
    "        \"\"\"\n",
    "        return self.class_map_inv[emotion]\n",
    "    \n",
    "    def num_to_label(self, emotion):\n",
    "        \"\"\"\n",
    "            Decode a numerical value (e.g. 0) to a label (e.g. \"angry\")\n",
    "        \"\"\"\n",
    "        return self.class_map[emotion]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 69403\n",
      "zipped_train = [(tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0000, 0.0000, 0.0000]]), 5), (tensor([[-0.0061, -0.0113, -0.0058,  ...,  0.0000,  0.0000,  0.0000]]), 3), (tensor([[0.3028, 0.4946, 0.4370,  ..., 0.0000, 0.0000, 0.0000]]), 5), (tensor([[-9.3498e-05, -9.3497e-05, -9.3497e-05,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 3), (tensor([[0.0010, 0.0007, 0.0014,  ..., 0.0000, 0.0000, 0.0000]]), 2), (tensor([[0.0014, 0.0014, 0.0014,  ..., 0.0003, 0.0022, 0.0007]]), 2), (tensor([[3.0010e-05, 3.0010e-05, 3.0010e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]), 5), (tensor([[-1.5681e-07, -1.5681e-07, -1.5681e-07,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 0), (tensor([[-0.0031,  0.0015,  0.0069,  ...,  0.0000,  0.0000,  0.0000]]), 5), (tensor([[ 1.2033e-03, -5.4103e-05,  3.9915e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 5), (tensor([[-1.6306e-06, -1.6306e-06, -1.6306e-06,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 4), (tensor([[ 4.5305e-04, -3.1993e-05,  1.3470e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 4), (tensor([[0.0007, 0.0007, 0.0007,  ..., 0.0000, 0.0000, 0.0000]]), 4), (tensor([[-8.5214e-06, -8.5214e-06, -8.5214e-06,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 3), (tensor([[-0.0002, -0.0002, -0.0002,  ...,  0.0000,  0.0000,  0.0000]]), 1), (tensor([[0.0125, 0.0203, 0.0054,  ..., 0.0000, 0.0000, 0.0000]]), 3), (tensor([[4.8175e-04, 5.4649e-06, 5.6402e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]), 2), (tensor([[0.0042, 0.0070, 0.0064,  ..., 0.0000, 0.0000, 0.0000]]), 2), (tensor([[0.0005, 0.0019, 0.0009,  ..., 0.0000, 0.0000, 0.0000]]), 3), (tensor([[-0.0003, -0.0003, -0.0003,  ...,  0.0000,  0.0000,  0.0000]]), 3), (tensor([[0.0115, 0.0143, 0.0143,  ..., 0.0000, 0.0000, 0.0000]]), 2), (tensor([[-0.0007, -0.0007, -0.0007,  ...,  0.0000,  0.0000,  0.0000]]), 0), (tensor([[-0.0248, -0.0414, -0.0364,  ...,  0.0000,  0.0000,  0.0000]]), 5), (tensor([[-2.6653e-03, -1.8061e-04,  5.0496e-05,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 1), (tensor([[-0.0026, -0.0037, -0.0038,  ...,  0.0000,  0.0000,  0.0000]]), 0), (tensor([[ 0.0006, -0.0008, -0.0055,  ...,  0.0000,  0.0000,  0.0000]]), 4), (tensor([[0.0079, 0.0123, 0.0105,  ..., 0.0000, 0.0000, 0.0000]]), 1), (tensor([[-1.1986e-05, -1.1986e-05, -1.1986e-05,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 3), (tensor([[0.0003, 0.0003, 0.0003,  ..., 0.0000, 0.0000, 0.0000]]), 1), (tensor([[0.0034, 0.0082, 0.0053,  ..., 0.0000, 0.0000, 0.0000]]), 3), (tensor([[0.0096, 0.0120, 0.0048,  ..., 0.0000, 0.0000, 0.0000]]), 1), (tensor([[0.0022, 0.0026, 0.0031,  ..., 0.0000, 0.0000, 0.0000]]), 5), (tensor([[-6.7966e-06, -3.5937e-05,  2.5629e-06,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 4), (tensor([[-0.0049, -0.0039, -0.0047,  ...,  0.0000,  0.0000,  0.0000]]), 0), (tensor([[-0.0018, -0.0010,  0.0005,  ...,  0.0000,  0.0000,  0.0000]]), 5), (tensor([[5.3835e-05, 5.3835e-05, 5.3835e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]), 2), (tensor([[-0.0031, -0.0030, -0.0035,  ...,  0.0000,  0.0000,  0.0000]]), 3), (tensor([[0.0006, 0.0005, 0.0007,  ..., 0.0000, 0.0000, 0.0000]]), 4), (tensor([[0.0006, 0.0006, 0.0006,  ..., 0.0000, 0.0000, 0.0000]]), 0), (tensor([[5.5392e-05, 5.5392e-05, 5.5392e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]), 2), (tensor([[0.0595, 0.1059, 0.0851,  ..., 0.0000, 0.0000, 0.0000]]), 1), (tensor([[-0.0019, -0.0029, -0.0022,  ...,  0.0000,  0.0000,  0.0000]]), 0), (tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0000, 0.0000, 0.0000]]), 1), (tensor([[-3.5717e-06, -3.5717e-06, -3.5717e-06,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 0), (tensor([[ 7.9815e-05, -5.6247e-04,  5.0427e-04,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 5), (tensor([[-0.0772, -0.1192, -0.1027,  ...,  0.0000,  0.0000,  0.0000]]), 3), (tensor([[0.0018, 0.0039, 0.0054,  ..., 0.0000, 0.0000, 0.0000]]), 4), (tensor([[-8.0494e-05, -8.0494e-05, -8.0494e-05,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 2), (tensor([[0.0046, 0.0092, 0.0099,  ..., 0.0000, 0.0000, 0.0000]]), 4), (tensor([[-0.0169, -0.0078,  0.0006,  ...,  0.0000,  0.0000,  0.0000]]), 0), (tensor([[-0.0011, -0.0011, -0.0011,  ...,  0.0000,  0.0000,  0.0000]]), 0), (tensor([[-9.6016e-05, -9.5933e-05, -9.6020e-05,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 4), (tensor([[-0.0068,  0.0553,  0.1670,  ...,  0.0000,  0.0000,  0.0000]]), 2), (tensor([[0.0055, 0.0087, 0.0074,  ..., 0.0000, 0.0000, 0.0000]]), 4), (tensor([[-0.0004, -0.0003, -0.0005,  ...,  0.0000,  0.0000,  0.0000]]), 1), (tensor([[0.0006, 0.0006, 0.0006,  ..., 0.0000, 0.0000, 0.0000]]), 2), (tensor([[5.9647e-05, 5.3222e-05, 6.0645e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]), 1), (tensor([[ 0.0010, -0.0024, -0.0149,  ...,  0.0000,  0.0000,  0.0000]]), 0), (tensor([[3.5509e-05, 3.5509e-05, 3.5509e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]), 5), (tensor([[-0.0013, -0.0116, -0.0017,  ...,  0.0000,  0.0000,  0.0000]]), 1)]\n"
     ]
    }
   ],
   "source": [
    "dataset = AudioEmotionsDataset(max_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.train_dataloader))\n",
    "print(f\"{dataset.X_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES: tensor([[[ 5.1163e-04,  1.8992e-03,  8.9940e-04,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-9.6016e-05, -9.5933e-05, -9.6020e-05,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]])\n",
      "LABELS: tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset.train_dataloader:\n",
    "    print(f\"FEATURES: {batch[0]}\")\n",
    "    print(f\"LABELS: {batch[1]}\")\n",
    "    # print(value)\n",
    "    break\n",
    "# iter(dataset.train_dataloader).next()\n",
    "\n",
    "# dataset.train_dataloader."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ling-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
